# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_-eWDuBx7CaHG2Ohhme1wzHaDQQ-eGl
"""

import torch
import numpy as np
import torch.nn.functional as F
import torch.nn as nn
import random

import math

import pandas as pd

from .sleep_model import simple_model
from .early_stopping import EarlyStopping

def model_training(training_generator, valid_generator, device, sleep_weight, wake_weight, pretrain=False):
  if pretrain:
    model = torch.load('models/pretrain_global_model/global_model.pt').to(device)
  else:
    model = simple_model(4).to(device)
  # Start training data
  optimizer = torch.optim.Adam(params=model.parameters())
  early_stopping = EarlyStopping(patience=10, verbose=True)
  valid_losses = []

  for i in range(150):
    for j, (X, y) in enumerate(training_generator):
      optimizer.zero_grad()

      probs = model(X.float().to(device)).float()
      loss = torch.nn.CrossEntropyLoss()(probs, y.type(torch.LongTensor).to(device))

      loss = torch.nn.CrossEntropyLoss(reduction='none')(probs, y.type(torch.LongTensor).to(device))
      loss[y==0] *= sleep_weight
      loss[y==1] *= wake_weight
      loss = torch.mean(loss)

      loss.backward()
      optimizer.step()

    model.eval()
    for j, (X, y) in enumerate(valid_generator):

      probs = model(X.float().to(device)).float()
      loss = torch.nn.CrossEntropyLoss()(probs, y.type(torch.LongTensor).to(device))

      loss = torch.nn.CrossEntropyLoss(reduction='none')(probs, y.type(torch.LongTensor).to(device))
      loss[y==0] *= sleep_weight
      loss[y==1] *= wake_weight
      loss = torch.mean(loss)

      valid_losses.append(loss.item())

    valid_loss = np.average(valid_losses)
    valid_losses = []

    early_stopping(valid_loss, model)
    if early_stopping.early_stop:
        print("Early stopping")
        break
  return model



